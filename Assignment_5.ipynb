{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdduiEYCnD8XFAiMbmgREw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChityalaRahul/Generative-AI_B40/blob/main/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RWhfVvScTWE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Define Activation Functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Step 2: Prepare Training and Test Data\n",
        "X_train = np.array([[0.2, 0.1], [0.3, 0.2], [0.4, 0.3], [0.7, 0.6], [0.8, 0.7], [0.9, 0.8]])\n",
        "y_train = np.array([[0.3441], [0.3500], [0.3558], [0.3729], [0.3785], [0.3841]])\n",
        "X_test = np.array([[0.5, 0.4], [0.6, 0.5]])\n",
        "y_test = np.array([[0.3615], [0.3672]])\n",
        "\n",
        "# Step 3: Define ANN Architecture\n",
        "input_layer_neurons = 2\n",
        "hidden_layer_neurons = 3\n",
        "output_layer_neurons = 1\n",
        "\n",
        "# Step 4: Initialize Weights and Biases randomly\n",
        "weights_input_hidden = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
        "bias_hidden = np.random.uniform(size=(1, hidden_layer_neurons))\n",
        "weights_hidden_output = np.random.uniform(size=(hidden_layer_neurons, output_layer_neurons))\n",
        "bias_output = np.random.uniform(size=(1, output_layer_neurons))\n",
        "\n",
        "# Learning parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Step 5: Train the Model\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    hidden_layer_activation = np.dot(X_train, weights_input_hidden) + bias_hidden\n",
        "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
        "\n",
        "    output_layer_activation = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "    predicted_output = output_layer_activation  # Linear activation for output\n",
        "\n",
        "    # Compute the error\n",
        "    error = y_train - predicted_output\n",
        "\n",
        "    # Backpropagation\n",
        "    d_predicted_output = error\n",
        "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Update weights and biases\n",
        "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
        "    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
        "    weights_input_hidden += X_train.T.dot(d_hidden_layer) * learning_rate\n",
        "    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    # Print the loss at every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(y_train - predicted_output))\n",
        "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "# Step 6: Calculate MSE for training and testing datasets\n",
        "mse_train = np.mean(np.square(y_train - predicted_output))\n",
        "print(f'Training MSE: {mse_train:.4f}')\n",
        "\n",
        "hidden_layer_activation_test = np.dot(X_test, weights_input_hidden) + bias_hidden\n",
        "hidden_layer_output_test = sigmoid(hidden_layer_activation_test)\n",
        "\n",
        "output_layer_activation_test = np.dot(hidden_layer_output_test, weights_hidden_output) + bias_output\n",
        "predicted_output_test = output_layer_activation_test  # Linear activation for output\n",
        "\n",
        "mse_test = np.mean(np.square(y_test - predicted_output_test))\n",
        "print(f'Test MSE: {mse_test:.4f}')\n",
        "\n",
        "# Step 7: Make Predictions for new user-input values\n",
        "def predict(x1, x2):\n",
        "    input_data = np.array([[x1, x2]])\n",
        "    hidden_layer_activation = np.dot(input_data, weights_input_hidden) + bias_hidden\n",
        "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
        "\n",
        "    output_layer_activation = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "    predicted_output = output_layer_activation  # Linear activation for output\n",
        "\n",
        "    return predicted_output\n",
        "\n",
        "# Read input data from user\n",
        "x1 = float(input(\"Enter x1: \"))\n",
        "x2 = float(input(\"Enter x2: \"))\n",
        "predicted_output_user = predict(x1, x2)\n",
        "print(f'Predicted Output for [{x1}, {x2}]: {predicted_output_user[0][0]:.4f}')\n"
      ]
    }
  ]
}